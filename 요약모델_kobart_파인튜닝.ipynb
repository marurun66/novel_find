{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# ✅ 1️⃣ 학습 데이터 로드\n",
    "with open(\"labeling_novels_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 2️⃣ 데이터셋 변환 (프롬프트 추가)\n",
    "train_data = [{\n",
    "    \"input\": f\"다음 문장 중 작가의 연혁과 수상이력은 제외하고 스토리부분만 추출 해주세요. 등장인물의 직업, 스토리 전개, 주요 사건을 포함해주세요. 스토리가 없을 경우 줄거리 없음으로 처리하세요.:\\n\\n{item['description']}\",\n",
    "    \"target\": item[\"summary\"]\n",
    "} for item in labeled_data]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"target\": [item[\"target\"] for item in train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 3️⃣ KoBART 모델 & 토크나이저 로드\n",
    "model_name = \"gogamza/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 4️⃣ 데이터 토크나이징 (Trainer가 이해할 수 있는 형태로 변환)\n",
    "def preprocess_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"], \n",
    "        max_length=1024, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # 🔹 \"target\"을 \"labels\"로 변환 후 토크나이징\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target\"], \n",
    "            max_length=250, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4ac102888645999208cd6315aa4d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 5️⃣ 데이터셋 변환 (batched=True로 속도 최적화)\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[\"input\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 6️⃣ 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart_summary_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",  # 🔹 매 epoch마다 평가\n",
    "    save_strategy=\"epoch\",        # 🔹 매 epoch마다 저장\n",
    "    per_device_train_batch_size=2,  # 🔹 배치 크기 (GPU 메모리에 따라 조정 가능)\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # 🔹 학습 횟수 (필요에 따라 조정 가능)\n",
    "    weight_decay=0.01,  # 🔹 가중치 감쇠 (과적합 방지)\n",
    "    save_total_limit=5,  # 🔹 체크포인트 최대 개수\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_70500/3899536195.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 7️⃣ 🤗 Datasets의 train_test_split() 사용\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# ✅ 8️⃣ train/eval 데이터셋 설정\n",
    "train_data = dataset_split[\"train\"]\n",
    "eval_data = dataset_split[\"test\"]\n",
    "\n",
    "# ✅ 9️⃣ Trainer 객체 생성 (eval_dataset 추가)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,  # 🔹 평가 데이터셋 추가!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 03:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.985400</td>\n",
       "      <td>0.673293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.692524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.722187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>0.735216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.7166613732261219, metrics={'train_runtime': 192.9941, 'train_samples_per_second': 4.508, 'train_steps_per_second': 2.254, 'total_flos': 530470718668800.0, 'train_loss': 0.7166613732261219, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 10 학습 시작 🚀\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KoBART 파인튜닝 완료! 모델이 './kobart_summary_finetuned'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 9️⃣ 모델 저장\n",
    "model.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "tokenizer.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "\n",
    "print(\"✅ KoBART 파인튜닝 완료! 모델이 './kobart_summary_finetuned'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 가장 좋은 모델이 './kobart_best_model'에 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "best_checkpoint = \"./kobart_summary_finetuned/checkpoint-174\"  # epoch 2 모델 체크포인트\n",
    "save_path = \"./kobart_best_model\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# 저장\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"✅ 가장 좋은 모델이 '{save_path}'에 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 원문:  먼 바다에서 펼쳐지는 노인의 고독한 사투!\n",
      "\n",
      "미국 현대 문학의 개척자라 불리는 어니스트 헤밍웨이의 대표작 『노인과 바다』. 퓰리처상 수상작이자 헤밍웨이의 마지막 소설로, 작가 고유의 소설 수법과 실존 철학이 집약된 헤밍웨이 문학의 결정판이다. 한 노인의 실존적 투쟁과 불굴의 의지를 절제된 문장으로 강렬하게 그려냈다. 십여 년 동안 이렇다 할 작품을 내놓지 못했던 헤밍웨이는 이 작품을 통해 작가적 생명력을 재확인하고 삶을 긍정하는 성숙한 태도를 보여주었다. 개인주의와 허무주의를 넘어 인간과 자연을 긍정하고 진정한 연대의 가치를 역설한다. 감정을 절제한 문체와 사실주의 기법, 다양한 상징과 전지적 화법을 활용하여 작품의 깊이를 더했다.\n",
      "🔹 요약 결과:  한 노인의 실존적 투쟁과 불굴의 의지를 절제된 문체로 강렬하게 그려낸 작품으로, 개인주의와 허무주의를 넘어 인간과 자연을 긍정하고 진정한 연대의 가치를 역설한다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# ✅ 저장된 모델 불러오기\n",
    "best_checkpoint = \"./kobart_best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# ✅ 테스트할 입력 문장\n",
    "test_text = \"먼 바다에서 펼쳐지는 노인의 고독한 사투!\\n\\n미국 현대 문학의 개척자라 불리는 어니스트 헤밍웨이의 대표작 『노인과 바다』. 퓰리처상 수상작이자 헤밍웨이의 마지막 소설로, 작가 고유의 소설 수법과 실존 철학이 집약된 헤밍웨이 문학의 결정판이다. 한 노인의 실존적 투쟁과 불굴의 의지를 절제된 문장으로 강렬하게 그려냈다. 십여 년 동안 이렇다 할 작품을 내놓지 못했던 헤밍웨이는 이 작품을 통해 작가적 생명력을 재확인하고 삶을 긍정하는 성숙한 태도를 보여주었다. 개인주의와 허무주의를 넘어 인간과 자연을 긍정하고 진정한 연대의 가치를 역설한다. 감정을 절제한 문체와 사실주의 기법, 다양한 상징과 전지적 화법을 활용하여 작품의 깊이를 더했다.\"\n",
    "\n",
    "# ✅ 입력 문장을 토큰화\n",
    "input_ids = tokenizer(\n",
    "    test_text, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=1024, \n",
    "    truncation=True\n",
    "    \n",
    "    ).input_ids\n",
    "\n",
    "# ✅ 모델로 요약 생성\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=1.5,\n",
    "    temperature=0.8)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"🔹 원문: \", test_text)\n",
    "print(\"🔹 요약 결과: \", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "## summary \"\" 로 작업용 키값을 설정했더니 줄거리가 없다고 인식하는 경우가 많았음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
