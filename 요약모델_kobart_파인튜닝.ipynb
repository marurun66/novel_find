{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# âœ… 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ë¡œë“œ\n",
    "with open(\"labeling_novels_filtered.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labeled_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 2ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜ (í”„ë¡¬í”„íŠ¸ ì¶”ê°€)\n",
    "train_data = [{\n",
    "    \"input\": f\"ë‹¤ìŒ ë¬¸ì¥ ì¤‘ ì‘ê°€ì˜ ì—°í˜ê³¼ ìˆ˜ìƒì´ë ¥ì€ ì œì™¸í•˜ê³  ìŠ¤í† ë¦¬ë¶€ë¶„ë§Œ ì¶”ì¶œ í•´ì£¼ì„¸ìš”. ë“±ì¥ì¸ë¬¼ì˜ ì§ì—…, ìŠ¤í† ë¦¬ ì „ê°œ, ì£¼ìš” ì‚¬ê±´ì„ í¬í•¨í•´ì£¼ì„¸ìš”. ìŠ¤í† ë¦¬ê°€ ì—†ì„ ê²½ìš° ì¤„ê±°ë¦¬ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.:\\n\\n{item['description']}\",\n",
    "    \"target\": item[\"summary\"]\n",
    "} for item in labeled_data]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input\": [item[\"input\"] for item in train_data],\n",
    "    \"target\": [item[\"target\"] for item in train_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 3ï¸âƒ£ KoBART ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"gogamza/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… 4ï¸âƒ£ ë°ì´í„° í† í¬ë‚˜ì´ì§• (Trainerê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜)\n",
    "def preprocess_data(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"], \n",
    "        max_length=1024, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # ğŸ”¹ \"target\"ì„ \"labels\"ë¡œ ë³€í™˜ í›„ í† í¬ë‚˜ì´ì§•\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"target\"], \n",
    "            max_length=250, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4ac102888645999208cd6315aa4d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 5ï¸âƒ£ ë°ì´í„°ì…‹ ë³€í™˜ (batched=Trueë¡œ ì†ë„ ìµœì í™”)\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True, remove_columns=[\"input\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hi/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 6ï¸âƒ£ í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobart_summary_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",  # ğŸ”¹ ë§¤ epochë§ˆë‹¤ í‰ê°€\n",
    "    save_strategy=\"epoch\",        # ğŸ”¹ ë§¤ epochë§ˆë‹¤ ì €ì¥\n",
    "    per_device_train_batch_size=2,  # ğŸ”¹ ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,  # ğŸ”¹ í•™ìŠµ íšŸìˆ˜ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)\n",
    "    weight_decay=0.01,  # ğŸ”¹ ê°€ì¤‘ì¹˜ ê°ì‡  (ê³¼ì í•© ë°©ì§€)\n",
    "    save_total_limit=5,  # ğŸ”¹ ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ ê°œìˆ˜\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/cf10k4857715p70flsxw2g9m0000gn/T/ipykernel_70500/3899536195.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# âœ… 7ï¸âƒ£ ğŸ¤— Datasetsì˜ train_test_split() ì‚¬ìš©\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# âœ… 8ï¸âƒ£ train/eval ë°ì´í„°ì…‹ ì„¤ì •\n",
    "train_data = dataset_split[\"train\"]\n",
    "eval_data = dataset_split[\"test\"]\n",
    "\n",
    "# âœ… 9ï¸âƒ£ Trainer ê°ì²´ ìƒì„± (eval_dataset ì¶”ê°€)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,  # ğŸ”¹ í‰ê°€ ë°ì´í„°ì…‹ ì¶”ê°€!\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='435' max='435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [435/435 03:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.985400</td>\n",
       "      <td>0.673293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.692524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.722187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.213200</td>\n",
       "      <td>0.735216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=435, training_loss=0.7166613732261219, metrics={'train_runtime': 192.9941, 'train_samples_per_second': 4.508, 'train_steps_per_second': 2.254, 'total_flos': 530470718668800.0, 'train_loss': 0.7166613732261219, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# âœ… 10 í•™ìŠµ ì‹œì‘ ğŸš€\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# âœ… 9ï¸âƒ£ ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "tokenizer.save_pretrained(\"./kobart_summary_finetuned\")\n",
    "\n",
    "print(\"âœ… KoBART íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ './kobart_summary_finetuned'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ './kobart_best_model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "best_checkpoint = \"./kobart_summary_finetuned/checkpoint-174\"  # epoch 2 ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸\n",
    "save_path = \"./kobart_best_model\"\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# ì €ì¥\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"âœ… ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì´ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ì›ë¬¸:  ë¨¼ ë°”ë‹¤ì—ì„œ í¼ì³ì§€ëŠ” ë…¸ì¸ì˜ ê³ ë…í•œ ì‚¬íˆ¬!\n",
      "\n",
      "ë¯¸êµ­ í˜„ëŒ€ ë¬¸í•™ì˜ ê°œì²™ìë¼ ë¶ˆë¦¬ëŠ” ì–´ë‹ˆìŠ¤íŠ¸ í—¤ë°ì›¨ì´ì˜ ëŒ€í‘œì‘ ã€ë…¸ì¸ê³¼ ë°”ë‹¤ã€. í“°ë¦¬ì²˜ìƒ ìˆ˜ìƒì‘ì´ì í—¤ë°ì›¨ì´ì˜ ë§ˆì§€ë§‰ ì†Œì„¤ë¡œ, ì‘ê°€ ê³ ìœ ì˜ ì†Œì„¤ ìˆ˜ë²•ê³¼ ì‹¤ì¡´ ì² í•™ì´ ì§‘ì•½ëœ í—¤ë°ì›¨ì´ ë¬¸í•™ì˜ ê²°ì •íŒì´ë‹¤. í•œ ë…¸ì¸ì˜ ì‹¤ì¡´ì  íˆ¬ìŸê³¼ ë¶ˆêµ´ì˜ ì˜ì§€ë¥¼ ì ˆì œëœ ë¬¸ì¥ìœ¼ë¡œ ê°•ë ¬í•˜ê²Œ ê·¸ë ¤ëƒˆë‹¤. ì‹­ì—¬ ë…„ ë™ì•ˆ ì´ë ‡ë‹¤ í•  ì‘í’ˆì„ ë‚´ë†“ì§€ ëª»í–ˆë˜ í—¤ë°ì›¨ì´ëŠ” ì´ ì‘í’ˆì„ í†µí•´ ì‘ê°€ì  ìƒëª…ë ¥ì„ ì¬í™•ì¸í•˜ê³  ì‚¶ì„ ê¸ì •í•˜ëŠ” ì„±ìˆ™í•œ íƒœë„ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. ê°œì¸ì£¼ì˜ì™€ í—ˆë¬´ì£¼ì˜ë¥¼ ë„˜ì–´ ì¸ê°„ê³¼ ìì—°ì„ ê¸ì •í•˜ê³  ì§„ì •í•œ ì—°ëŒ€ì˜ ê°€ì¹˜ë¥¼ ì—­ì„¤í•œë‹¤. ê°ì •ì„ ì ˆì œí•œ ë¬¸ì²´ì™€ ì‚¬ì‹¤ì£¼ì˜ ê¸°ë²•, ë‹¤ì–‘í•œ ìƒì§•ê³¼ ì „ì§€ì  í™”ë²•ì„ í™œìš©í•˜ì—¬ ì‘í’ˆì˜ ê¹Šì´ë¥¼ ë”í–ˆë‹¤.\n",
      "ğŸ”¹ ìš”ì•½ ê²°ê³¼:  í•œ ë…¸ì¸ì˜ ì‹¤ì¡´ì  íˆ¬ìŸê³¼ ë¶ˆêµ´ì˜ ì˜ì§€ë¥¼ ì ˆì œëœ ë¬¸ì²´ë¡œ ê°•ë ¬í•˜ê²Œ ê·¸ë ¤ë‚¸ ì‘í’ˆìœ¼ë¡œ, ê°œì¸ì£¼ì˜ì™€ í—ˆë¬´ì£¼ì˜ë¥¼ ë„˜ì–´ ì¸ê°„ê³¼ ìì—°ì„ ê¸ì •í•˜ê³  ì§„ì •í•œ ì—°ëŒ€ì˜ ê°€ì¹˜ë¥¼ ì—­ì„¤í•œë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# âœ… ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "best_checkpoint = \"./kobart_best_model\"\n",
    "model = BartForConditionalGeneration.from_pretrained(best_checkpoint)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(best_checkpoint)\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸í•  ì…ë ¥ ë¬¸ì¥\n",
    "test_text = \"ë¨¼ ë°”ë‹¤ì—ì„œ í¼ì³ì§€ëŠ” ë…¸ì¸ì˜ ê³ ë…í•œ ì‚¬íˆ¬!\\n\\në¯¸êµ­ í˜„ëŒ€ ë¬¸í•™ì˜ ê°œì²™ìë¼ ë¶ˆë¦¬ëŠ” ì–´ë‹ˆìŠ¤íŠ¸ í—¤ë°ì›¨ì´ì˜ ëŒ€í‘œì‘ ã€ë…¸ì¸ê³¼ ë°”ë‹¤ã€. í“°ë¦¬ì²˜ìƒ ìˆ˜ìƒì‘ì´ì í—¤ë°ì›¨ì´ì˜ ë§ˆì§€ë§‰ ì†Œì„¤ë¡œ, ì‘ê°€ ê³ ìœ ì˜ ì†Œì„¤ ìˆ˜ë²•ê³¼ ì‹¤ì¡´ ì² í•™ì´ ì§‘ì•½ëœ í—¤ë°ì›¨ì´ ë¬¸í•™ì˜ ê²°ì •íŒì´ë‹¤. í•œ ë…¸ì¸ì˜ ì‹¤ì¡´ì  íˆ¬ìŸê³¼ ë¶ˆêµ´ì˜ ì˜ì§€ë¥¼ ì ˆì œëœ ë¬¸ì¥ìœ¼ë¡œ ê°•ë ¬í•˜ê²Œ ê·¸ë ¤ëƒˆë‹¤. ì‹­ì—¬ ë…„ ë™ì•ˆ ì´ë ‡ë‹¤ í•  ì‘í’ˆì„ ë‚´ë†“ì§€ ëª»í–ˆë˜ í—¤ë°ì›¨ì´ëŠ” ì´ ì‘í’ˆì„ í†µí•´ ì‘ê°€ì  ìƒëª…ë ¥ì„ ì¬í™•ì¸í•˜ê³  ì‚¶ì„ ê¸ì •í•˜ëŠ” ì„±ìˆ™í•œ íƒœë„ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. ê°œì¸ì£¼ì˜ì™€ í—ˆë¬´ì£¼ì˜ë¥¼ ë„˜ì–´ ì¸ê°„ê³¼ ìì—°ì„ ê¸ì •í•˜ê³  ì§„ì •í•œ ì—°ëŒ€ì˜ ê°€ì¹˜ë¥¼ ì—­ì„¤í•œë‹¤. ê°ì •ì„ ì ˆì œí•œ ë¬¸ì²´ì™€ ì‚¬ì‹¤ì£¼ì˜ ê¸°ë²•, ë‹¤ì–‘í•œ ìƒì§•ê³¼ ì „ì§€ì  í™”ë²•ì„ í™œìš©í•˜ì—¬ ì‘í’ˆì˜ ê¹Šì´ë¥¼ ë”í–ˆë‹¤.\"\n",
    "\n",
    "# âœ… ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”\n",
    "input_ids = tokenizer(\n",
    "    test_text, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=1024, \n",
    "    truncation=True\n",
    "    \n",
    "    ).input_ids\n",
    "\n",
    "# âœ… ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "summary_ids = model.generate(\n",
    "    input_ids, \n",
    "    max_length=200, \n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=1.5,\n",
    "    temperature=0.8)\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ğŸ”¹ ì›ë¬¸: \", test_text)\n",
    "print(\"ğŸ”¹ ìš”ì•½ ê²°ê³¼: \", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "## summary \"\" ë¡œ ì‘ì—…ìš© í‚¤ê°’ì„ ì„¤ì •í–ˆë”ë‹ˆ ì¤„ê±°ë¦¬ê°€ ì—†ë‹¤ê³  ì¸ì‹í•˜ëŠ” ê²½ìš°ê°€ ë§ì•˜ìŒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
